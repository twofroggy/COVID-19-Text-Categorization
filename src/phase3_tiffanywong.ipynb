{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Phase 3 \n",
    "Tiffany Wong \n",
    "\n",
    "### Description \n",
    "In the final phase of our class project, you will use the datasets you created in phase 2 to build a text categorization model.\n",
    "\n",
    "### Note to self \n",
    "- PRIMARY = twitter-stance \n",
    "- SECONDARY = changeorg-stance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal \n",
    "The goal of this final phase of the project is to build a text categorization model on your primary dataset, and to evaluate it on both your primary and your secondary dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data Partitioning \n",
    "Create a Training set for your model by randomly selecting 70% of the texts in your PRIMARY dataset.  Use the remaining 30% of texts from the PRIMARY dataset as your Test (PRIMARY) set.  Designate 100% of your SECONDARY dataset as the Test (SECONDARY) dataset.  So you should have one Training set (drawn from the PRIMARY data), and two different Test sets (one from PRIMARY and one from SECONDARY). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import sklearn \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in primary and secondary datasets \n",
    "twitter_stance = pd.read_csv('/Users/tiffwong/Desktop/cs585/project/twitter_stance_labels2.csv') \n",
    "changeorg_stance = pd.read_csv('/Users/tiffwong/Desktop/cs585/project/changeorg_stance_labels.csv') \n",
    "twitter_stance = twitter_stance.drop(['Unnamed: 0'], axis=1)\n",
    "changeorg_stance = changeorg_stance.drop(['Unnamed: 0'], axis=1) \n",
    "\n",
    "# sanity check \n",
    "# changeorg_stance.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "twittertext_train, twittertext_test, twitterlabel_train, twitterlabel_test = train_test_split(twitter_stance[\"text\"], twitter_stance[\"label\"],\n",
    "                                   random_state=1, \n",
    "                                   train_size=0.70, \n",
    "                                   shuffle=True) \n",
    "\n",
    "# sanity check\n",
    "len(twittertext_train)/len(twitter_stance)\n",
    "\n",
    "# join twittertext_train and twitterlabel_train\n",
    "twitter_train = pd.concat([twittertext_train, twitterlabel_train], axis=1, join='inner') \n",
    "# join twittertext_test and twitterlabel_test \n",
    "twitter_test = pd.concat([twittertext_test, twitterlabel_test], axis=1, join='inner') \n",
    "\n",
    "# rename changeorg_stance dataset to be 100% test \n",
    "changetext_test = changeorg_stance[\"text\"] \n",
    "changelabel_test = changeorg_stance[\"label\"]\n",
    "\n",
    "# twitter_test.head() \n",
    "# changetext_test.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataframes right now: \n",
    "\n",
    "* PRIMARY training \n",
    "\n",
    "twitter_train = 70% of twitter_stance dataset for training \n",
    "\n",
    "* PRIMARY testing \n",
    "\n",
    "twitter_test = 30% of twitter_stance dataset for testing \n",
    "\n",
    "* SECONDARY testing \n",
    "\n",
    "changeorg_test = 100% of changeorg_stance dataset for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Baseline model training: \n",
    "Train a simple bag-of-words classifier on your Training dataset.  If your data comes from the stance task, you will build a multiclass model (one which can assign one of three labels - pro-mitigation, anti-mitigation, or unclear). \n",
    "\n",
    "\n",
    "An example of how to use scikit-learn to build a simple text categorization model is here: https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1260, 5841)"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer() \n",
    "twitter_train_counts = count_vect.fit_transform(twittertext_train) \n",
    "twitter_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1260, 5841)"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_transformer = TfidfTransformer(use_idf=False).fit(twitter_train_counts) \n",
    "twitter_train_tf = tf_transformer.transform(twitter_train_counts) \n",
    "twitter_train_tf.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1260, 5841)"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer() \n",
    "twitter_train_tfidf = tfidf_transformer.fit_transform(twitter_train_counts) \n",
    "twitter_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(twitter_train_tfidf, twitterlabel_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model evaluation 1: \n",
    "Calculate your baseline model's accuracy for your model's predictions on the Test (PRIMARY) set, and on the Test (SECONDARY) set.  Enter these values in the answer boxes provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a pipeline \n",
    "In order to make the vectorizer => transformer => classifier easier to work with, scikit-learn provides a Pipeline class that behaves like a compound classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1145    A local #pharmaceutical company had expressed ...\n",
       "927     200K dead isn’t enough for you? You’re gonna r...\n",
       "1189    After vaccine announcement, \\ngovt. announces ...\n",
       "1065    Please don't put tracking nano-bots in our Cov...\n",
       "671     Leave Fauci alone. Talk about Trump and Pence,...\n",
       "                              ...                        \n",
       "905     #CovidVaccine will never work. ANd has never w...\n",
       "1791    Go America! Time to unite against the real ene...\n",
       "1096    The CDC announced Friday the relaxing of COVID...\n",
       "235     When something doesn't make logical sense. Lik...\n",
       "1061    According to Wong late on Friday, details of t...\n",
       "Name: text, Length: 1260, dtype: object"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twittertext_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-19 {color: black;background-color: white;}#sk-container-id-19 pre{padding: 0;}#sk-container-id-19 div.sk-toggleable {background-color: white;}#sk-container-id-19 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-19 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-19 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-19 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-19 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-19 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-19 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-19 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-19 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-19 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-19 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-19 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-19 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-19 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-19 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-19 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-19 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-19 div.sk-item {position: relative;z-index: 1;}#sk-container-id-19 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-19 div.sk-item::before, #sk-container-id-19 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-19 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-19 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-19 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-19 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-19 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-19 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-19 div.sk-label-container {text-align: center;}#sk-container-id-19 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-19 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-19\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, CountVectorizer()), (&#x27;tfidf&#x27;, TfidfTransformer()),\n",
       "                (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-73\" type=\"checkbox\" ><label for=\"sk-estimator-id-73\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, CountVectorizer()), (&#x27;tfidf&#x27;, TfidfTransformer()),\n",
       "                (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-74\" type=\"checkbox\" ><label for=\"sk-estimator-id-74\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-75\" type=\"checkbox\" ><label for=\"sk-estimator-id-75\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-76\" type=\"checkbox\" ><label for=\"sk-estimator-id-76\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()), \n",
    "    ('tfidf', TfidfTransformer()), \n",
    "    ('clf', MultinomialNB()),\n",
    "]) \n",
    "text_clf.fit(twittertext_train, twitterlabel_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRIMARY dataset evaluation (twitter_stance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use twittertext_test and twitterlabel_test \n",
    "docs_test = twittertext_test\n",
    "predicted = text_clf.predict(docs_test) \n",
    "twitter_accuracy = np.mean(predicted == twitterlabel_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SECONDARY dataset evaluation (changeorg_stance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use twittertext_test and twitterlabel_test \n",
    "docs_test = changetext_test \n",
    "predicted = text_clf.predict(docs_test) \n",
    "change_accuracy = np.mean(predicted == changelabel_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature engineering: \n",
    "In order to try to improve your model, think about what features of the text might be associated with the category you are trying to predict.  What attributes of a text besides the presence of individual words might be good predictors (for example, regular expression patterns or specific word sequences)?  Create at least three new features that represent attributes of the text.  Add them to your model and retrain. \n",
    "\n",
    "An example of how to add a set of features (defined as a vector of 1/0 values indicating whether the attribute is present or absent for a given text) is shown here: https://gist.github.com/DerrickHiggins/20c77745b080e3d493231424d7da9a2f "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 1: Word Count \n",
    "One of the new features can be word count. Seeing if the length of a text is any indication of the stance/label of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_count(string): \n",
    "    wrdcount = len(string.split(\" \")) \n",
    "    return(wrdcount) \n",
    "\n",
    "word_count(twittertext_train[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 2: Sentiment Analysis - subjectivity\n",
    "The sentiment property returns a namedtuple of the form Sentiment(polarity, subjectivity). The polarity score is a float within the range [-1.0, 1.0]. The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective. \n",
    "\n",
    "I will be adding each text's subjectivity and polarity rating as two new features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textblob\n",
    "from textblob import TextBlob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_subjectivity(string): \n",
    "    text = TextBlob(string) \n",
    "    subjectivity = text.sentiment.subjectivity \n",
    "    return(subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Excellent thread on CDC and CDPH school guidelines.  Prioritizing students in school is a great place to start'"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twittertext_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.875"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_subjectivity(twittertext_train[1])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 3: Sentiment Analysis - Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_polarity(string): \n",
    "    text = TextBlob(string) \n",
    "    polarity = text.sentiment.polarity \n",
    "    return(polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_polarity(twittertext_train[1])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 3: Number of stopwords \n",
    "Stopwords are simply insignificant words that need to be filtered out before text processing. \n",
    "\n",
    "Third feature to add is the number of stopwords in each text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tiffwong/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/tiffwong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "nltk.download('stopwords') \n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_stopwords(string):\n",
    "    stop_words = set(stopwords.words('english'))  \n",
    "    word_tokens = word_tokenize(string)\n",
    "    stopwords_x = [w for w in word_tokens if w in stop_words]\n",
    "    return len(stopwords_x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_stopwords(twitter_stance[\"text\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntwittertext_train, twittertext_test, twitterlabel_train, twitterlabel_test = train_test_split(twitter_stance[\"text\"], twitter_stance[\"label\"],\\n                                   random_state=1, \\n                                   train_size=0.70, \\n                                   shuffle=True) \\n\\n# sanity check\\nlen(twittertext_train)/len(twitter_stance)\\n\\n# join twittertext_train and twitterlabel_train\\ntwitter_train = pd.concat([twittertext_train, twitterlabel_train], axis=1, join=\\'inner\\') \\n# join twittertext_test and twitterlabel_test \\ntwitter_test = pd.concat([twittertext_test, twitterlabel_test], axis=1, join=\\'inner\\') \\n\\n# rename changeorg_stance dataset to be 100% test \\nchangetext_test = changeorg_stance[\"text\"] \\nchangelabel_test = changeorg_stance[\"label\"]\\n'"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "twittertext_train, twittertext_test, twitterlabel_train, twitterlabel_test = train_test_split(twitter_stance[\"text\"], twitter_stance[\"label\"],\n",
    "                                   random_state=1, \n",
    "                                   train_size=0.70, \n",
    "                                   shuffle=True) \n",
    "\n",
    "# sanity check\n",
    "len(twittertext_train)/len(twitter_stance)\n",
    "\n",
    "# join twittertext_train and twitterlabel_train\n",
    "twitter_train = pd.concat([twittertext_train, twitterlabel_train], axis=1, join='inner') \n",
    "# join twittertext_test and twitterlabel_test \n",
    "twitter_test = pd.concat([twittertext_test, twitterlabel_test], axis=1, join='inner') \n",
    "\n",
    "# rename changeorg_stance dataset to be 100% test \n",
    "changetext_test = changeorg_stance[\"text\"] \n",
    "changelabel_test = changeorg_stance[\"label\"]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate new features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_features(dataframe1): \n",
    "    return_df = pd.DataFrame({'index':dataframe1.index, 'text':dataframe1.values}) \n",
    "    # word count \n",
    "    return_df['word_count'] = return_df['text'].apply(lambda x: word_count(x)) \n",
    "\n",
    "    # subjectivity \n",
    "    return_df['subjectivity'] = return_df['text'].apply(lambda x: sentiment_subjectivity(x)) \n",
    "\n",
    "    # polarity and add 1 so all values get shifted up by 1, bc polarity has range=[-1.1] \n",
    "    return_df['polarity'] = return_df['text'].apply(lambda x: sentiment_polarity(x)) \n",
    "    return_df['polarity'] = return_df['polarity'] + 1\n",
    "\n",
    "    # stopword count\n",
    "    return_df['stopwords'] = return_df['text'].apply(lambda x: count_stopwords(x))  \n",
    "\n",
    "    return return_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### twittertext_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>polarity</th>\n",
       "      <th>stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1145</td>\n",
       "      <td>A local #pharmaceutical company had expressed ...</td>\n",
       "      <td>17</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>927</td>\n",
       "      <td>200K dead isn’t enough for you? You’re gonna r...</td>\n",
       "      <td>16</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1189</td>\n",
       "      <td>After vaccine announcement, \\ngovt. announces ...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1065</td>\n",
       "      <td>Please don't put tracking nano-bots in our Cov...</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>671</td>\n",
       "      <td>Leave Fauci alone. Talk about Trump and Pence,...</td>\n",
       "      <td>34</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               text  word_count  \\\n",
       "0   1145  A local #pharmaceutical company had expressed ...          17   \n",
       "1    927  200K dead isn’t enough for you? You’re gonna r...          16   \n",
       "2   1189  After vaccine announcement, \\ngovt. announces ...          10   \n",
       "3   1065  Please don't put tracking nano-bots in our Cov...          16   \n",
       "4    671  Leave Fauci alone. Talk about Trump and Pence,...          34   \n",
       "\n",
       "   subjectivity  polarity  stopwords  \n",
       "0        0.1875    0.9375          5  \n",
       "1        0.4500    0.9000          7  \n",
       "2        0.0000    1.0000          1  \n",
       "3        0.0000    1.0000          6  \n",
       "4        0.0000    1.0000         11  "
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twittertext_train_wfeatures = new_features(twittertext_train)\n",
    "twittertext_train_wfeatures.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### twittertext_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>polarity</th>\n",
       "      <th>stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1462</td>\n",
       "      <td>#coronavirus #Covid #COVID19 #COVIDー19 #CovidV...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>510</td>\n",
       "      <td>I wouldn’t take a vaccine from any of them unt...</td>\n",
       "      <td>15</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>612</td>\n",
       "      <td>Safety &amp;amp; efficacy of #CovidVaccine must fo...</td>\n",
       "      <td>14</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1322</td>\n",
       "      <td>to produce up to an additional 100 million #Co...</td>\n",
       "      <td>17</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>993</td>\n",
       "      <td>CDC won’t take blame for companies who force m...</td>\n",
       "      <td>50</td>\n",
       "      <td>0.227679</td>\n",
       "      <td>1.040179</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               text  word_count  \\\n",
       "0   1462  #coronavirus #Covid #COVID19 #COVIDー19 #CovidV...          12   \n",
       "1    510  I wouldn’t take a vaccine from any of them unt...          15   \n",
       "2    612  Safety &amp; efficacy of #CovidVaccine must fo...          14   \n",
       "3   1322  to produce up to an additional 100 million #Co...          17   \n",
       "4    993  CDC won’t take blame for companies who force m...          50   \n",
       "\n",
       "   subjectivity  polarity  stopwords  \n",
       "0      0.000000  1.000000          2  \n",
       "1      0.200000  0.850000          8  \n",
       "2      0.100000  1.000000          1  \n",
       "3      0.300000  1.000000          7  \n",
       "4      0.227679  1.040179         11  "
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twittertext_test_wfeatures = new_features(twittertext_test)\n",
    "twittertext_test_wfeatures.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### changetext_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>polarity</th>\n",
       "      <th>stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>CARTA PELOS POBRES!</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Target: Protect your team and community as Cov...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Restrict Maximum Monthy BCHydro Charges During...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Ensure Sir Richard Branson sells private asset...</td>\n",
       "      <td>14</td>\n",
       "      <td>0.375</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Cancel Lucknow University Exams! Mass promotio...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               text  word_count  \\\n",
       "0      0                                CARTA PELOS POBRES!           3   \n",
       "1      1  Target: Protect your team and community as Cov...          10   \n",
       "2      2  Restrict Maximum Monthy BCHydro Charges During...           8   \n",
       "3      3  Ensure Sir Richard Branson sells private asset...          14   \n",
       "4      4  Cancel Lucknow University Exams! Mass promotio...          11   \n",
       "\n",
       "   subjectivity  polarity  stopwords  \n",
       "0         0.000       1.0          0  \n",
       "1         0.000       1.0          4  \n",
       "2         0.000       1.0          0  \n",
       "3         0.375       1.0          5  \n",
       "4         0.000       1.0          3  "
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "changetext_test_wfeatures = new_features(changetext_test)\n",
    "changetext_test_wfeatures.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain model with new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1260, 5841)"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer() \n",
    "twitter_train_counts = count_vect.fit_transform(twittertext_train) \n",
    "tfidf_transformer = TfidfTransformer() \n",
    "twitter_train_tfidf = tfidf_transformer.fit_transform(twitter_train_counts) \n",
    "twitter_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new feature to the document representation \n",
    "# combine feature arrays \n",
    "with_features = sparse.hstack([twitter_train_tfidf, twittertext_train_wfeatures[['word_count', \"subjectivity\", \"polarity\", \"stopwords\"]]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrain the model \n",
    "clf = MultinomialNB().fit(with_features, twitterlabel_train)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model evaluation 2: \n",
    "Calculate overall model accuracy for your new model's predictions on the Test (PRIMARY) set, and on the Test (SECONDARY) set.  Enter these values in the answer boxes provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRIMARY dataset evaluation (twitter_stance)\n",
    "using twittertext_test to transform and twitterlabel_test to test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_test_counts = count_vect.transform(twittertext_test) \n",
    "twitter_test_tfidf = tfidf_transformer.transform(primary_test_counts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use twittertext_test_wfeatures and twitterlabel_test \n",
    "# combine test and new features \n",
    "docs_test = sparse.hstack([twitter_test_tfidf, twittertext_test_wfeatures[['word_count','subjectivity', 'polarity', 'stopwords']]]) \n",
    "\n",
    "# predict labels \n",
    "predicted = clf.predict(docs_test) \n",
    "\n",
    "# calculate accuracy \n",
    "twitter_accuracy2 = np.mean(predicted == twitterlabel_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SECONDARY dataset evaluation (changeorg_stance)\n",
    "using changetext_test to transform and changelabel_test to test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondary_test_counts = count_vect.transform(changetext_test) \n",
    "change_test_tfidf = tfidf_transformer.transform(secondary_test_counts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use changetext_test_wfeatures and changelabel_test \n",
    "# combine test and new features \n",
    "docs_test = sparse.hstack([change_test_tfidf, changetext_test_wfeatures[['word_count','subjectivity', 'polarity', 'stopwords']]]) \n",
    "\n",
    "# predict labels \n",
    "predicted = clf.predict(docs_test) \n",
    "\n",
    "# calculate accuracy \n",
    "change_accuracy2 = np.mean(predicted == changelabel_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'With the first model, the primary dataset had 48.33% accuracy, while with the second model, it had 44.07% accuracy.'"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"With the first model, the primary dataset had {twitter_accuracy}% accuracy, while with the second model, it had {twitter_accuracy2}% accuracy.\".format(twitter_accuracy=round(twitter_accuracy*100, 2), twitter_accuracy2=round(twitter_accuracy2*100, 2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'With the first model, the secondary dataset had 43.87% accuracy, while with the second model, it had 46.07% accuracy.'"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"With the first model, the secondary dataset had {change_accuracy}% accuracy, while with the second model, it had {change_accuracy2}% accuracy.\".format(change_accuracy=round(change_accuracy*100, 2), change_accuracy2=round(change_accuracy2*100, 2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "535"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(changeorg_stance['label']=='unclear').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "761"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(twitter_stance['label']=='unclear').sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
